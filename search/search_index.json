{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About DTrack Disturbance Track (DTrack) offers 24/7 monitoring that uses Machine Learning (AI) to identify disturbing noises, such as dog barking or car alarms. Rather than using expensive \"cloud\" infrastructure for advanced categorization, DTrack creates a model that is trained to detect only the exact noise that it was trained on. This helps eliminate false positives and decreases the effort required to perform matching. How It Works Set up the Monitoring Device Collect some initial recordings Train a model from collected recordings Use trained model for automatic detection Why It Exists In some jurisdictions, understaffing leads to complete dismissal of any call that is not life threatening. In these cases, even a detailed report of ongoing noise disturbances may be entirely dismissed without a full 48 hours of detailed (to-the-minute) logging of every single occurrence, alongside submitted video. Painstakingly listening to this audio in order to accurately log each and every disturbance, unsurprisingly, adds to overall frustration, but it also yields a report that is guaranteed to be taken seriously by a District Attorney. DTrack exists to provide an initial report that can be easily reviewed in much less time. How Is This Possible? Most scoffs at this endevour have come from an understanding that large datasets are huge and take a tremendous amount of power to work with, making it impossible to keep detection entirely within lightweight hardware. This comes from a fundamental misunderstanding of what detection is actually needed. Consider which of the following questions is easier for a computer to answer: Is a banana present in this image? Is a whole, pristine, ripe, yellow banana present in this image? The former can include any type of banana. It could still be green, or perhaps yellow but cut in half, or brown and smashed. At what point of decay is it no longer a banana? Large data models take all of these questions into consideration and then try to apply tags that answer yes or no to each question. Disturbance Tracker only cares about the tag that it was trained to detect, which eliminates extra overhead and makes detection a very lightweight operation.","title":"About DTrack"},{"location":"#about-dtrack","text":"Disturbance Track (DTrack) offers 24/7 monitoring that uses Machine Learning (AI) to identify disturbing noises, such as dog barking or car alarms. Rather than using expensive \"cloud\" infrastructure for advanced categorization, DTrack creates a model that is trained to detect only the exact noise that it was trained on. This helps eliminate false positives and decreases the effort required to perform matching.","title":"About DTrack"},{"location":"#how-it-works","text":"Set up the Monitoring Device Collect some initial recordings Train a model from collected recordings Use trained model for automatic detection","title":"How It Works"},{"location":"#why-it-exists","text":"In some jurisdictions, understaffing leads to complete dismissal of any call that is not life threatening. In these cases, even a detailed report of ongoing noise disturbances may be entirely dismissed without a full 48 hours of detailed (to-the-minute) logging of every single occurrence, alongside submitted video. Painstakingly listening to this audio in order to accurately log each and every disturbance, unsurprisingly, adds to overall frustration, but it also yields a report that is guaranteed to be taken seriously by a District Attorney. DTrack exists to provide an initial report that can be easily reviewed in much less time.","title":"Why It Exists"},{"location":"#how-is-this-possible","text":"Most scoffs at this endevour have come from an understanding that large datasets are huge and take a tremendous amount of power to work with, making it impossible to keep detection entirely within lightweight hardware. This comes from a fundamental misunderstanding of what detection is actually needed. Consider which of the following questions is easier for a computer to answer: Is a banana present in this image? Is a whole, pristine, ripe, yellow banana present in this image? The former can include any type of banana. It could still be green, or perhaps yellow but cut in half, or brown and smashed. At what point of decay is it no longer a banana? Large data models take all of these questions into consideration and then try to apply tags that answer yes or no to each question. Disturbance Tracker only cares about the tag that it was trained to detect, which eliminates extra overhead and makes detection a very lightweight operation.","title":"How Is This Possible?"},{"location":"setup/configure/","text":"Configuration File DTrack has defaults that are designed to function adequately on a Raspberry Pi 5, although these make very poor monitoring devices. If your Selected Hardware provides a hardware video en conder, then it is important to modify record_save_options (below). config.json Configuration is stored in a JSON file, called config.json . Important JSON Tips: 1. Format: { \"OPTION\": VALUE, \"OPTION\": VALUE, \"OPTION\": VALUE } 2. All VALUE s are one of: string : \"anythin inside of quotes\" list : [\"like\", \"a\", \"string\", \"broken\", \"into\", \"parts\"] number : 5 or 3.1415 boolean : true for Yes or false for No 3. Every VALUE must have a trailing comma ( , ), except the last must not. Test-Driven Defaults Defaults were selected based on tests using lowest-recommended hardware . Encoder Documentation (Upstream Defaults) Value Purpose -t <time> Must be set on each input device -tune zerolatency This is required for software encoding on low-end cpu -bufsize 64M Very large buffer to helps avoid processing spikes -crf 23 Default is best; 25 reduces size by 30%, but 24 creates movement -maxrate 3M Hard quality limit, based on NO hardware encoding * 1080p recordings will see quality improvement up to about 7M * Larger value creates larger files and XBUF for CPU-only encoding -preset fast Yield fewest XRUN errors -framerate 15 Fast enough to catch most movement Baseline Command: ffmpeg -y -loglevel warning -nostdin -nostats \\ -t 00:10:00 -f alsa -i plughw \\ -t 00:10:00 -f v4l2 -i /dev/video0 \\ -map 0:a -c:a pcm_s16le -ar 48000 -ac 1 -f wav - \\ -filter_complex [1:v]...[dtstamp] -map 0:a -map [dtstamp] \\ -c:a pcm_s16le -ar 48000 -ac 1 -c:v libx264 baseline.mkv \\ >/dev/null","title":"Configuration File"},{"location":"setup/configure/#configuration-file","text":"DTrack has defaults that are designed to function adequately on a Raspberry Pi 5, although these make very poor monitoring devices. If your Selected Hardware provides a hardware video en conder, then it is important to modify record_save_options (below).","title":"Configuration File"},{"location":"setup/configure/#configjson","text":"Configuration is stored in a JSON file, called config.json . Important JSON Tips: 1. Format: { \"OPTION\": VALUE, \"OPTION\": VALUE, \"OPTION\": VALUE } 2. All VALUE s are one of: string : \"anythin inside of quotes\" list : [\"like\", \"a\", \"string\", \"broken\", \"into\", \"parts\"] number : 5 or 3.1415 boolean : true for Yes or false for No 3. Every VALUE must have a trailing comma ( , ), except the last must not.","title":"config.json"},{"location":"setup/configure/#test-driven-defaults","text":"Defaults were selected based on tests using lowest-recommended hardware . Encoder Documentation (Upstream Defaults) Value Purpose -t <time> Must be set on each input device -tune zerolatency This is required for software encoding on low-end cpu -bufsize 64M Very large buffer to helps avoid processing spikes -crf 23 Default is best; 25 reduces size by 30%, but 24 creates movement -maxrate 3M Hard quality limit, based on NO hardware encoding * 1080p recordings will see quality improvement up to about 7M * Larger value creates larger files and XBUF for CPU-only encoding -preset fast Yield fewest XRUN errors -framerate 15 Fast enough to catch most movement Baseline Command: ffmpeg -y -loglevel warning -nostdin -nostats \\ -t 00:10:00 -f alsa -i plughw \\ -t 00:10:00 -f v4l2 -i /dev/video0 \\ -map 0:a -c:a pcm_s16le -ar 48000 -ac 1 -f wav - \\ -filter_complex [1:v]...[dtstamp] -map 0:a -map [dtstamp] \\ -c:a pcm_s16le -ar 48000 -ac 1 -c:v libx264 baseline.mkv \\ >/dev/null","title":"Test-Driven Defaults"},{"location":"setup/hardware/","text":"Hardware Selection The financial investment required for this project comes down to hardware selection. The hardware required: Training Device: Typically the same laptop being used to set things up Monitoring Device: Low-power mini-PC with a few USB ports Microphone: The sensor that detects disturbing audio Camera: The sensor that shows proof of audio source Training Device A laptop (or desktop) of any age will work great for creating a trained model. CUDA-supporting GPUs, like Nvidia, can improve training time by up to 20%. Any currently-owned laptop with a webcam and microphone is likely a great option to try out this project, before deciding to purchase any additional hardware. Monitoring Device Although much of DTrack can easily run on a Raspberry Pi, these devices lack a \"hardware video en coder\", making video generation a very intensive process. This limits recording to about 3M/s, which is the extreme low side of 1080p. Fortunately, there are countless mini-PCs with hardware en coders that can tackle high-quality 4k video with ease, making storage size the next bottleneck. Any mini-PC with one of the following CPUs should be great picks: Intel Celeron (N150+) Intel Gemini Lake (N4000+) Intel Jasper Lake (N5000+) AMD Ryzen Embedded (R2000+, 7000+, 8000+) Microphone: The trained model used for automatic detection will be created using audio samples that were created from this microphone. This makes it one of the most important hardware decisions for this project. Changing this will require recording new samples in order to train a new model. Many microphones are designed to eliminate audio that is likely to be clasified as disturbances, such as dog barks or car alarms. It is important to find a microphone that does not advertise features like noise cancelling and instead offers \"high sensitivity\" and a \"wide frequency response.\" Some great options include: Angetube USB Microphone Blue Snowflake Camera: The camera you choose must be able to capture enough detail to identify the origin of the noise, but that is all the detail that is required. There is no reason to find an expensive camera. Most modern webcams that claim 1080p will be capable of capturing enough detail to accompany a written report.","title":"Hardware Selection"},{"location":"setup/hardware/#hardware-selection","text":"The financial investment required for this project comes down to hardware selection. The hardware required: Training Device: Typically the same laptop being used to set things up Monitoring Device: Low-power mini-PC with a few USB ports Microphone: The sensor that detects disturbing audio Camera: The sensor that shows proof of audio source","title":"Hardware Selection"},{"location":"setup/hardware/#training-device","text":"A laptop (or desktop) of any age will work great for creating a trained model. CUDA-supporting GPUs, like Nvidia, can improve training time by up to 20%. Any currently-owned laptop with a webcam and microphone is likely a great option to try out this project, before deciding to purchase any additional hardware.","title":"Training Device"},{"location":"setup/hardware/#monitoring-device","text":"Although much of DTrack can easily run on a Raspberry Pi, these devices lack a \"hardware video en coder\", making video generation a very intensive process. This limits recording to about 3M/s, which is the extreme low side of 1080p. Fortunately, there are countless mini-PCs with hardware en coders that can tackle high-quality 4k video with ease, making storage size the next bottleneck. Any mini-PC with one of the following CPUs should be great picks: Intel Celeron (N150+) Intel Gemini Lake (N4000+) Intel Jasper Lake (N5000+) AMD Ryzen Embedded (R2000+, 7000+, 8000+) Microphone: The trained model used for automatic detection will be created using audio samples that were created from this microphone. This makes it one of the most important hardware decisions for this project. Changing this will require recording new samples in order to train a new model. Many microphones are designed to eliminate audio that is likely to be clasified as disturbances, such as dog barks or car alarms. It is important to find a microphone that does not advertise features like noise cancelling and instead offers \"high sensitivity\" and a \"wide frequency response.\" Some great options include: Angetube USB Microphone Blue Snowflake Camera: The camera you choose must be able to capture enough detail to identify the origin of the noise, but that is all the detail that is required. There is no reason to find an expensive camera. Most modern webcams that claim 1080p will be capable of capturing enough detail to accompany a written report.","title":"Monitoring Device"},{"location":"setup/install/","text":"Installing DTrack Todo This project was originally written in Python. The transition to golang began because of performance issues, but became moot after a few magical ffmpeg/x264 options were discovered. There is a chance that tfgo will not have enough support for this project, but we shall see ... holding off on this document until the project finally reaches beta.","title":"Easy Installation"},{"location":"setup/install/#installing-dtrack","text":"Todo This project was originally written in Python. The transition to golang began because of performance issues, but became moot after a few magical ffmpeg/x264 options were discovered. There is a chance that tfgo will not have enough support for this project, but we shall see ... holding off on this document until the project finally reaches beta.","title":"Installing DTrack"},{"location":"setup/options/","text":"Configuration Options Visit the Configuration File section for information about how to use these configuration options. Workspace This is the location where DTrack stores data. This includes saved recordings, audio clips, trained models, temporary data, etc. Default Value: \"_workspace/\" Type Configuration Variable Environment Variable string workspace DTRACK_WORKSPACE Workspace Keep Temp Keep temporary data after extracted for review. Default Value: false Type Configuration Variable Environment Variable boolean keep_temp DTRACK_KEEP_TEMP Record Audio Device The device identifier used to record audio. Default Value: \"plughw\" Type Configuration Variable Environment Variable string audio_device RECORD_AUDIO_DEVICE List audio devices: ffmpeg -loglevel warning -sources alsa Sample Output: Auto-detected sources for alsa: null [Discard all samples (playback) or generate zero samples (capture)] hw:CARD=Snowflake,DEV=0 [Direct hardware device without any conversions] plughw:CARD=Snowflake,DEV=0 [Hardware device with all software conversions] default:CARD=Snowflake [Default Audio Device] sysdefault:CARD=Snowflake [Default Audio Device] front:CARD=Snowflake,DEV=0 [Front output / input] dsnoop:CARD=Snowflake,DEV=0 [Direct sample snooping device] Look for devices with: \" software conversions \" It is likely worth taking time to test available options. Record Audio Options Additional ffmpeg options used for audio capture. Default Value: [ \"-f\", \"alsa\" ] Type Configuration Variable Environment Variable list audio_options RECORD_AUDIO_OPTIONS Record Video Device The device identifier used to record video. Default Value: \"/dev/video0\" Type Configuration Variable Environment Variable string video_device RECORD_VIDEO_DEVICE List video devices: ls -1 /dev/video* Sample Output: /dev/video0 /dev/video1 /dev/video2 /dev/video3 List capabilities: ffmpeg -hide_banner -f v4l2 -list_formats all -i <DEVICE> Sample Output: [v4l2 @ 0x5612] Compressed: mjpeg: Motion-JPEG : 640x360 640x480 960x540 1024x576 1280x720 1920x1080 2560x1440 3840x2160 [v4l2 @ 0x5612] Compressed: h264: H.264 : 640x360 640x480 960x540 1024x576 1280x720 1920x1080 2560x1440 3840x2160 [v4l2 @ 0x5612] Compressed: Unsupported: HEVC : 640x360 640x480 960x540 1024x576 1280x720 1920x1080 2560x1440 3840x2160 [v4l2 @ 0x5612] Raw : yuyv422: YUYV 4:2:2 : 640x360 640x480 960x540 1024x576 1280x720 1920x1080 [in#0 @ 0x5612] Error opening input: Immediate exit requested Error opening input file /dev/video2. This output shows a camera capable of recording 4K video. Record Video Options Additional ffmpeg options used for video capture. Default Value: [ \"-f\", \"v4l2\", \"-framerate\", \"15\" ] Type Configuration Variable Environment Variable list video_options RECORD_VIDEO_OPTIONS Record Video Timestamp Long string that defines the datestamp that is added on top of recorded video. The default value uses Monospace Bold , set to a bright Red, 48-Point font. This font is provided by the fonts-freefont-ttf package, This filter is applied to every frame, so it is possible to add hflip,vflip, to the start of the string, which will result in an image that is flipped both horizontally and vertically--perfect for an upside-down camera. Default Value: \"drawtext=fontfile=/usr/share/fonts/truetype/freefont/FreeMonoBold.ttf:text=%{localtime}:fontcolor=red@0.9:x=7:y=7:fontsize=48\" Type Configuration Variable Environment Variable string video_timestamp RECORD_VIDEO_TIMESTAMP Record Video Advanced Video creation arguments that ultimately control the final recorded video quality. A sensible range for 1080p video is between 3M and 8M , with 3.5M being the observed upper limit on a Raspberry Pi 5--due to lack of hardware en coder. Default Value: [ \"libx264\", \"-crf\", \"23\", \"-preset\", \"fast\", \"-tune\", \"zerolatency\", \"-maxrate\", \"3M\", \"-bufsize\", \"24M\" ] Type Configuration Variable Environment Variable list video_advanced RECORD_VIDEO_ADVANCED Record Inspect Models List of tags used for training, and trained models used for automatic detection. Default Value: [ ] Type Configuration Variable Environment Variable list inspect_models RECORD_INSPECT_MODELS Record Inspect Backlog Defines the maximum number of \"segments\" that can back up in each queue before newly recorded audio is discarded from queue. Default Value: 5 Type Configuration Variable Environment Variable number inspect_backlog RECORD_INSPECT_BACKLOG Record Duration Length of time to use for each recording. Very short recordings will suffer inadequate compression and are harder to review. Very long recordings will be much larger and more difficult to submit. Default Value: \"00:10:00\" Type Configuration Variable Environment Variable string record_duration RECORD_DURATION Train Target Success rate of model (as percent correct) required before training is considered a success. Default Value: 0.95 Type Configuration Variable Environment Variable number train_target TRAIN_TARGET Train Rate One of the \"big three\" macro adjustments in machine learning. Default Value: 0.001 Type Configuration Variable Environment Variable number train_rate TRAIN_RATE Train Momentum One of the \"big three\" macro adjustments in machine learning. Default Value: 0.9 Type Configuration Variable Environment Variable number train_momentum TRAIN_MOMENTUM Train Dropout One of the \"big three\" macro adjustments in machine learning. Default Value: 0.2 Type Configuration Variable Environment Variable number train_dropout TRAIN_DROPOUT","title":"Configuration Options"},{"location":"setup/options/#configuration-options","text":"Visit the Configuration File section for information about how to use these configuration options.","title":"Configuration Options"},{"location":"setup/options/#workspace","text":"This is the location where DTrack stores data. This includes saved recordings, audio clips, trained models, temporary data, etc. Default Value: \"_workspace/\" Type Configuration Variable Environment Variable string workspace DTRACK_WORKSPACE","title":"Workspace"},{"location":"setup/options/#workspace-keep-temp","text":"Keep temporary data after extracted for review. Default Value: false Type Configuration Variable Environment Variable boolean keep_temp DTRACK_KEEP_TEMP","title":"Workspace Keep Temp"},{"location":"setup/options/#record-audio-device","text":"The device identifier used to record audio. Default Value: \"plughw\" Type Configuration Variable Environment Variable string audio_device RECORD_AUDIO_DEVICE List audio devices: ffmpeg -loglevel warning -sources alsa Sample Output: Auto-detected sources for alsa: null [Discard all samples (playback) or generate zero samples (capture)] hw:CARD=Snowflake,DEV=0 [Direct hardware device without any conversions] plughw:CARD=Snowflake,DEV=0 [Hardware device with all software conversions] default:CARD=Snowflake [Default Audio Device] sysdefault:CARD=Snowflake [Default Audio Device] front:CARD=Snowflake,DEV=0 [Front output / input] dsnoop:CARD=Snowflake,DEV=0 [Direct sample snooping device] Look for devices with: \" software conversions \" It is likely worth taking time to test available options.","title":"Record Audio Device"},{"location":"setup/options/#record-audio-options","text":"Additional ffmpeg options used for audio capture. Default Value: [ \"-f\", \"alsa\" ] Type Configuration Variable Environment Variable list audio_options RECORD_AUDIO_OPTIONS","title":"Record Audio Options"},{"location":"setup/options/#record-video-device","text":"The device identifier used to record video. Default Value: \"/dev/video0\" Type Configuration Variable Environment Variable string video_device RECORD_VIDEO_DEVICE List video devices: ls -1 /dev/video* Sample Output: /dev/video0 /dev/video1 /dev/video2 /dev/video3 List capabilities: ffmpeg -hide_banner -f v4l2 -list_formats all -i <DEVICE> Sample Output: [v4l2 @ 0x5612] Compressed: mjpeg: Motion-JPEG : 640x360 640x480 960x540 1024x576 1280x720 1920x1080 2560x1440 3840x2160 [v4l2 @ 0x5612] Compressed: h264: H.264 : 640x360 640x480 960x540 1024x576 1280x720 1920x1080 2560x1440 3840x2160 [v4l2 @ 0x5612] Compressed: Unsupported: HEVC : 640x360 640x480 960x540 1024x576 1280x720 1920x1080 2560x1440 3840x2160 [v4l2 @ 0x5612] Raw : yuyv422: YUYV 4:2:2 : 640x360 640x480 960x540 1024x576 1280x720 1920x1080 [in#0 @ 0x5612] Error opening input: Immediate exit requested Error opening input file /dev/video2. This output shows a camera capable of recording 4K video.","title":"Record Video Device"},{"location":"setup/options/#record-video-options","text":"Additional ffmpeg options used for video capture. Default Value: [ \"-f\", \"v4l2\", \"-framerate\", \"15\" ] Type Configuration Variable Environment Variable list video_options RECORD_VIDEO_OPTIONS","title":"Record Video Options"},{"location":"setup/options/#record-video-timestamp","text":"Long string that defines the datestamp that is added on top of recorded video. The default value uses Monospace Bold , set to a bright Red, 48-Point font. This font is provided by the fonts-freefont-ttf package, This filter is applied to every frame, so it is possible to add hflip,vflip, to the start of the string, which will result in an image that is flipped both horizontally and vertically--perfect for an upside-down camera. Default Value: \"drawtext=fontfile=/usr/share/fonts/truetype/freefont/FreeMonoBold.ttf:text=%{localtime}:fontcolor=red@0.9:x=7:y=7:fontsize=48\" Type Configuration Variable Environment Variable string video_timestamp RECORD_VIDEO_TIMESTAMP","title":"Record Video Timestamp"},{"location":"setup/options/#record-video-advanced","text":"Video creation arguments that ultimately control the final recorded video quality. A sensible range for 1080p video is between 3M and 8M , with 3.5M being the observed upper limit on a Raspberry Pi 5--due to lack of hardware en coder. Default Value: [ \"libx264\", \"-crf\", \"23\", \"-preset\", \"fast\", \"-tune\", \"zerolatency\", \"-maxrate\", \"3M\", \"-bufsize\", \"24M\" ] Type Configuration Variable Environment Variable list video_advanced RECORD_VIDEO_ADVANCED","title":"Record Video Advanced"},{"location":"setup/options/#record-inspect-models","text":"List of tags used for training, and trained models used for automatic detection. Default Value: [ ] Type Configuration Variable Environment Variable list inspect_models RECORD_INSPECT_MODELS","title":"Record Inspect Models"},{"location":"setup/options/#record-inspect-backlog","text":"Defines the maximum number of \"segments\" that can back up in each queue before newly recorded audio is discarded from queue. Default Value: 5 Type Configuration Variable Environment Variable number inspect_backlog RECORD_INSPECT_BACKLOG","title":"Record Inspect Backlog"},{"location":"setup/options/#record-duration","text":"Length of time to use for each recording. Very short recordings will suffer inadequate compression and are harder to review. Very long recordings will be much larger and more difficult to submit. Default Value: \"00:10:00\" Type Configuration Variable Environment Variable string record_duration RECORD_DURATION","title":"Record Duration"},{"location":"setup/options/#train-target","text":"Success rate of model (as percent correct) required before training is considered a success. Default Value: 0.95 Type Configuration Variable Environment Variable number train_target TRAIN_TARGET","title":"Train Target"},{"location":"setup/options/#train-rate","text":"One of the \"big three\" macro adjustments in machine learning. Default Value: 0.001 Type Configuration Variable Environment Variable number train_rate TRAIN_RATE","title":"Train Rate"},{"location":"setup/options/#train-momentum","text":"One of the \"big three\" macro adjustments in machine learning. Default Value: 0.9 Type Configuration Variable Environment Variable number train_momentum TRAIN_MOMENTUM","title":"Train Momentum"},{"location":"setup/options/#train-dropout","text":"One of the \"big three\" macro adjustments in machine learning. Default Value: 0.2 Type Configuration Variable Environment Variable number train_dropout TRAIN_DROPOUT","title":"Train Dropout"},{"location":"usage/collect/","text":"Data Collection Automatic detection cannot happen without a trained model, and a model cannot be trained without some initial data, recorded from our detection hardware. This loop is primed using audio data taken from your recording hardware. Critical! Changing the recording device, capture band, or even band can have a dramatic result on the ability of a trained model to continue accurate detection. 1. Begin collecting recordings: Begin continuous recording with: dtrack -a monitor -v Stop recording with: # From the same terminal session to cancel when current recording finishes Ctrl+C # Press a second time to disregard current recording and exit immediately Ctrl+C Recordings will be saved to ./_workspace/rotating/ . Demonstration Note: Clapping hands together is a great demonstration exercise. This can be set in config.yml with inspect_models: [clap] .","title":"Collect Recordings"},{"location":"usage/collect/#data-collection","text":"Automatic detection cannot happen without a trained model, and a model cannot be trained without some initial data, recorded from our detection hardware. This loop is primed using audio data taken from your recording hardware. Critical! Changing the recording device, capture band, or even band can have a dramatic result on the ability of a trained model to continue accurate detection. 1. Begin collecting recordings: Begin continuous recording with: dtrack -a monitor -v Stop recording with: # From the same terminal session to cancel when current recording finishes Ctrl+C # Press a second time to disregard current recording and exit immediately Ctrl+C Recordings will be saved to ./_workspace/rotating/ . Demonstration Note: Clapping hands together is a great demonstration exercise. This can be set in config.yml with inspect_models: [clap] .","title":"Data Collection"},{"location":"usage/inspect/","text":"Manual Inspection After model.{pth,wav} are generated, the inspect action can be used to manually review individual video ( .mkv ) files. python3 -m train.check -M $model_name -i $path_to_mkv This returns a list of frames where the trained noise was detected. These frames can then be reviewed/tagged using the review utility and then used train an improved model.","title":"Inspect Results"},{"location":"usage/inspect/#manual-inspection","text":"After model.{pth,wav} are generated, the inspect action can be used to manually review individual video ( .mkv ) files. python3 -m train.check -M $model_name -i $path_to_mkv This returns a list of frames where the trained noise was detected. These frames can then be reviewed/tagged using the review utility and then used train an improved model.","title":"Manual Inspection"},{"location":"usage/overview/","text":"Using DTrack DTrack is essentially broken into ... 1. Data Collection (monitor without model) 2. Data Analysis (review) 3. Model Training (train) 4. Manual Inspection (interact with model) 5. Automatic Reporting (monitor with model) To Do ...","title":"General Overview"},{"location":"usage/overview/#using-dtrack","text":"DTrack is essentially broken into ... 1. Data Collection (monitor without model) 2. Data Analysis (review) 3. Model Training (train) 4. Manual Inspection (interact with model) 5. Automatic Reporting (monitor with model) To Do ...","title":"Using DTrack"},{"location":"usage/report/","text":"Automatic Reporting TODO","title":"Automatic Reports"},{"location":"usage/report/#automatic-reporting","text":"TODO","title":"Automatic Reporting"},{"location":"usage/review/","text":"Data Analysis During initial data collection, it can be useful to set record_duration: to 2-5 minutes and then rename each recording as they complete, using the following as an example: 2024-08-10_13:54:00_TRAIN-clap.mkv 2024-08-10_13:58:17_TRAIN-clap.mkv 2024-08-10_14:00:26_TEST-nomatch.mkv 2024-08-10_14:02:38_TEST-nomatch.mkv 2024-08-10_14:04:57_TEST-nomatch.mkv 2024-08-10_14:07:20_TEST-nomatch.mkv 2024-08-10_14:09:45_TEST-clap.mkv 2024-08-10_14:17:07_TEST-nomatch.mkv 2024-08-10_14:19:35_TEST-nomatch.mkv 2024-08-10_14:22:02_TEST-clap.mkv TRAIN files were created with many variations of the sound being searched for, using different background noise, volumes, etc. These files will be cut into 1-second clips for model training. TEST files include variation, but may have only one instance of a sound being searched for--one needle in the haystack. These will be used to test the quality of each ML iteration. Once data is collected, it can be retrieved from _workspace/rotating on the recording device and copied to the same _workspace/rotating location on the device used for training . Testing Data: Test data is essentially the same as training data, except it is collected with the intent of being used only for testing. Follow the process for tagging and then move data to _workspace/test/<model>/ or _workspace/test/nomatch/ : Ultimately, these videos will be used to determine the accuracy of each model. Training Data: In order to determine if something is or is not , the source audio must be broken up into short consumable segments and segments matching the target model must be reviewed and saved (tagged) manually. Project Timing DTrack is designed for generating reports. Report granularity uses 1-minute cycles. 1 clap or 999 claps within 1 minute is logged as one hit. Each recording is broken into 2-second clips. Each clip overlaps the next by 1 second to prevent dead zones Open and review captured (from rotating/ ) using the inspection tool: dtrack -a review The review option provides a GUI to help simplify the process of reviewing and tagging 1-second clips. Keyboard Shortcuts: Left/Right: Navigate 1 frame left or right PgUp/PgDn: Navigate 60 frames left or right Home/End: Navigate to start or end Up: Replay audio clip","title":"Review Clips"},{"location":"usage/review/#data-analysis","text":"During initial data collection, it can be useful to set record_duration: to 2-5 minutes and then rename each recording as they complete, using the following as an example: 2024-08-10_13:54:00_TRAIN-clap.mkv 2024-08-10_13:58:17_TRAIN-clap.mkv 2024-08-10_14:00:26_TEST-nomatch.mkv 2024-08-10_14:02:38_TEST-nomatch.mkv 2024-08-10_14:04:57_TEST-nomatch.mkv 2024-08-10_14:07:20_TEST-nomatch.mkv 2024-08-10_14:09:45_TEST-clap.mkv 2024-08-10_14:17:07_TEST-nomatch.mkv 2024-08-10_14:19:35_TEST-nomatch.mkv 2024-08-10_14:22:02_TEST-clap.mkv TRAIN files were created with many variations of the sound being searched for, using different background noise, volumes, etc. These files will be cut into 1-second clips for model training. TEST files include variation, but may have only one instance of a sound being searched for--one needle in the haystack. These will be used to test the quality of each ML iteration. Once data is collected, it can be retrieved from _workspace/rotating on the recording device and copied to the same _workspace/rotating location on the device used for training . Testing Data: Test data is essentially the same as training data, except it is collected with the intent of being used only for testing. Follow the process for tagging and then move data to _workspace/test/<model>/ or _workspace/test/nomatch/ : Ultimately, these videos will be used to determine the accuracy of each model. Training Data: In order to determine if something is or is not , the source audio must be broken up into short consumable segments and segments matching the target model must be reviewed and saved (tagged) manually. Project Timing DTrack is designed for generating reports. Report granularity uses 1-minute cycles. 1 clap or 999 claps within 1 minute is logged as one hit. Each recording is broken into 2-second clips. Each clip overlaps the next by 1 second to prevent dead zones Open and review captured (from rotating/ ) using the inspection tool: dtrack -a review The review option provides a GUI to help simplify the process of reviewing and tagging 1-second clips. Keyboard Shortcuts: Left/Right: Navigate 1 frame left or right PgUp/PgDn: Navigate 60 frames left or right Home/End: Navigate to start or end Up: Replay audio clip","title":"Data Analysis"},{"location":"usage/train/","text":"Model Training Training a model is essentially a continuous loop of making a random model and then comparing it's effectiveness to the current best. Note If a model already exists, it will be used to prime the training routine. After all testing and training data is prepared, training can be initiated with: python3 -m train.model -M $model -y $tagged_clips -n $NO_MATCH_clips This will continue until target_accuracy (from config.yml ) is met. python3 -m train.model -M clap -y _workspace/tagged/clap -n _workspace/tagged/no-match INFO:Training iteration 1 INFO:Overall accuracy[1] is 50.0 INFO:Accuracy increased; keeping new model INFO:Training iteration 2 INFO:Overall accuracy[2] is 50.0 INFO:Accuracy worse than #1; discarding new INFO:Training iteration 3 [...] INFO:Training iteration 11 INFO:Overall accuracy[11] is 84.84848484848484 INFO:Accuracy worse than #9; discarding new INFO:Training iteration 12 INFO:Overall accuracy[12] is 86.36363636363637 INFO:Accuracy increased; keeping new model INFO:TRAINING COMPLETE :: Final Accuracy: 86.36363636363637 The final products of this training process are model.pth and model.wav . These two files can be copied into another workspace and then used for content inspection (detection) .","title":"Train a Model"},{"location":"usage/train/#model-training","text":"Training a model is essentially a continuous loop of making a random model and then comparing it's effectiveness to the current best. Note If a model already exists, it will be used to prime the training routine. After all testing and training data is prepared, training can be initiated with: python3 -m train.model -M $model -y $tagged_clips -n $NO_MATCH_clips This will continue until target_accuracy (from config.yml ) is met. python3 -m train.model -M clap -y _workspace/tagged/clap -n _workspace/tagged/no-match INFO:Training iteration 1 INFO:Overall accuracy[1] is 50.0 INFO:Accuracy increased; keeping new model INFO:Training iteration 2 INFO:Overall accuracy[2] is 50.0 INFO:Accuracy worse than #1; discarding new INFO:Training iteration 3 [...] INFO:Training iteration 11 INFO:Overall accuracy[11] is 84.84848484848484 INFO:Accuracy worse than #9; discarding new INFO:Training iteration 12 INFO:Overall accuracy[12] is 86.36363636363637 INFO:Accuracy increased; keeping new model INFO:TRAINING COMPLETE :: Final Accuracy: 86.36363636363637 The final products of this training process are model.pth and model.wav . These two files can be copied into another workspace and then used for content inspection (detection) .","title":"Model Training"}]}